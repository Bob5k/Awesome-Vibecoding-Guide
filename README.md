# The Comprehensive Vibecoding Guide
### From a practitioner for practitioners

A compendium of knowledge drawn from commercial project experience and hundreds of thousands of lines of AI-assisted code

---

## Table of Contents

1. Introduction
2. Tools and Tech Stack
3. Context Management
4. Workflow and Process
5. Lessons Learned
6. Cost Optimization
7. Philosophy and Best Practices
8. Case Studies and Real Projects

---

## Introduction

Vibecoding is an approach to software creation where you collaborate with AI as a coding partner. This guide is based on real commercial experience—projects delivered for clients where 98–100% of code was generated by AI under human supervision and direction.

### Who am I?

- Head of QA with over 10 years in software development
- 6 years as Head of Quality Assurance (process and engineering)
- Freelancer vibecoder—building software for local businesses
- AI experience since the early GPT beta
- Delivering commercial projects for companies with €100M–€1B annual revenue (mainly e-commerce: Magento, Shopify)

### What will you learn?

- How to save ~70% on AI tool costs without sacrificing quality
- How to manage context effectively in long-term projects
- How to avoid pitfalls and the most common vibecoding mistakes
- How to build a reliable tool stack tailored to your needs
- How to deliver commercial projects powered entirely by AI

---

## Tools and Tech Stack

### My Core Stack (Current – 2025)

#### 1. Zed.dev – Primary IDE

Why Zed?
- AI-native IDE—built from the ground up for AI work
- Lightweight—uses only 20% of VS Code’s resources for similar tasks
- Excellent context management—built-in AI agent keeps context clean
- Easy integration—connect to any LLM via API
- Agent features—great for longer tasks with automatic tracking

Key resource management advantages:
- Efficiently utilize up to 85% of the LLM’s token capacity
- No plugin clutter—clean context
- When a conversation nears the limit, the agent automatically compresses the dialog while preserving intent

#### 2. GLM Coding Plan – Main LLM

Pricing:
- Pro: $3/month – 120 prompts per 5 hours
- Max: ~$36/year – 2400 prompts per 5 hours
- Practicality of limits: Very hard to hit even with 2–5 parallel agents

Why GLM?
- Lowest cost while keeping quality around Sonnet 3.5 level
- State-of-the-art open-source model
- Can autonomously resolve bugs for 10–15 minutes without intervention
- Handles complex projects (Next.js + complex database + backend)

Cost comparison:
- Claude Code Max20: €200+ per month (with EU VAT)
- GLM Max: $36/year ≈ $3/month
- Savings: ~70%

#### 3. OpenSpec CLI – Specification Framework

Advantages:
- Completely free
- Easy integration with existing codebases (edge over GitHub Speckit)
- Specification-driven development
- Works with any LLM via Zed

Replaced in my stack: Traycer.ai (saving $25/month)

#### 4. Context7 MCP – Context Management

Features:
- Knowledge retention in long sessions
- Documentation indexing
- Connection to a documentation database
- Critical for long-term projects

Usage:
- All files in the /docs folder are indexed
- Context is pulled automatically when needed

#### 5. Devtools MCP – Testing & Debugging

Testing breakthrough:
- 95% less context usage than Playwright
- Intelligent navigation for web development tests
- Automatic debugging: console logs, navigation, 500 errors

Workflow:
- Launch the MCP
- Inform the agent about issues
- The agent autonomously gathers data, debugs, and resolves

Pro tip: It’s more efficient to tell GLM “use MCP” instead of writing a long debugging prompt.

### Complementary Stack

#### Claude Code CLI Client
- The best CLI client for Claude or any Anthropic endpoint
- Used for very complex tasks
- Alternative when detailed visibility of agent actions is needed

#### GitHub Speckit
- Excellent for new projects
- Integration issues with existing, complex codebases
- My pick for kickstarting clean projects
- After recent updates—highly recommended for pure vibecoding

### MCP Servers – Recommendations

I use:
- Context7—absolutely essential
- shadcN MCP—smooth integration without excessive context use
- Sequential Thinking MCP—context optimization
- Task Manager MCP—project organization

Note: Don’t use too many MCPs—they will quickly consume your context window!

### Tools I Dropped

#### Traycer.ai
Why I left:
- Too slow—planning takes long, validation too, fixes even longer
- Plugin dependency—requires VS Code/derivatives (issues with my repos)
- Costly—$25/month for realistic workloads
- Frustration—when the plan hallucinates, rollbacks waste lots of time

What was good:
- Automatic validation of ongoing development
- High output quality (when it works)

Conclusion: Achieve similar results with OpenSpec + GLM for a fraction of the price.

### Tool Compatibility

Not recommended to combine:
- Replit, Lovable, Bolt—closed platforms, limiting options
- Too many MCP servers at once
- Tools without the ability to connect using your own LLM API

---

## Context Management

### Fundamentals

The most important rule: Context is your most valuable resource. Manage it deliberately.

### Context Management Techniques

#### 1. Selective File Inclusion

Modular structure:
